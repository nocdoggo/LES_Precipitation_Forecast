{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18074d9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n",
      "1-D Lake Michigan data exists for  2006Fall  to  2007Spring\n",
      "2-D Lake Michigan data exists for  2006Fall  to  2007Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n",
      "100%|██████████| 4368/4368 [00:41<00:00, 105.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-D Lake Michigan data exists for  2007Fall  to  2008Spring\n",
      "2-D Lake Michigan data exists for  2007Fall  to  2008Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4392/4392 [00:42<00:00, 104.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-D Lake Michigan data exists for  2008Fall  to  2009Spring\n",
      "2-D Lake Michigan data exists for  2008Fall  to  2009Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4368/4368 [00:42<00:00, 103.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-D Lake Michigan data exists for  2009Fall  to  2010Spring\n",
      "2-D Lake Michigan data exists for  2009Fall  to  2010Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4368/4368 [00:42<00:00, 103.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n",
      "1-D Lake Michigan data exists for  2010Fall  to  2011Spring\n",
      "2-D Lake Michigan data exists for  2010Fall  to  2011Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n",
      "100%|██████████| 4368/4368 [00:57<00:00, 75.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-D Lake Michigan data exists for  2011Fall  to  2012Spring\n",
      "2-D Lake Michigan data exists for  2011Fall  to  2012Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4392/4392 [00:49<00:00, 88.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-D Lake Michigan data exists for  2012Fall  to  2013Spring\n",
      "2-D Lake Michigan data exists for  2012Fall  to  2013Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4368/4368 [00:41<00:00, 104.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n",
      "1-D Lake Michigan data exists for  2013Fall  to  2014Spring\n",
      "2-D Lake Michigan data exists for  2013Fall  to  2014Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n",
      "100%|██████████| 4368/4368 [00:41<00:00, 105.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n",
      "1-D Lake Michigan data exists for  2014Fall  to  2015Spring\n",
      "2-D Lake Michigan data exists for  2014Fall  to  2015Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n",
      "100%|██████████| 4368/4368 [00:41<00:00, 105.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n",
      "1-D Lake Michigan data exists for  2015Fall  to  2016Spring\n",
      "2-D Lake Michigan data exists for  2015Fall  to  2016Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n",
      "100%|██████████| 4392/4392 [00:41<00:00, 105.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather station data exists for  IL-14819-CHICAGO_MIDWAY_AP\n",
      "File exists, reading table!\n",
      "1-D Lake Michigan data exists for  2016Fall  to  2017Spring\n",
      "2-D Lake Michigan data exists for  2016Fall  to  2017Spring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448439/1927080630.py:176: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n",
      "100%|██████████| 4368/4368 [00:41<00:00, 105.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date as dtdt\n",
    "from pytz import timezone\n",
    "import itertools\n",
    "import math\n",
    "import glob\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "###############################################################################\n",
    "# Important, make sure to use the correct file\n",
    "weather_station = 'IL-14819-CHICAGO_MIDWAY_AP'\n",
    "station_ID_num = weather_station[3:8]\n",
    "###############################################################################\n",
    "# In this part, we select the number of years used for the experiment\n",
    "start_year_lib = [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]\n",
    "\n",
    "# start_year_lib = [2008, 2009, 2010, 2011, 2012, 2013, 2014]\n",
    "\n",
    "hour_lib = ['00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', \n",
    "            '10:00', '11:00', '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
    "            '18:00', '19:00', '20:00', '21:00', '22:00', '23:00']\n",
    "\n",
    "# Initialize an empty dataframe to store the combined data\n",
    "df_combined_all = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty list to store the lengths of each 'df_GOES_meteo_combined'\n",
    "lengths_list = []\n",
    "\n",
    "\n",
    "def daterange(date1, date2):\n",
    "    for n in range(int ((date2 - date1).days) + 1):\n",
    "        yield date1 + timedelta(n)\n",
    "\n",
    "###############################################################################\n",
    "def round_datetime_to_nearest_hour(obj_arr, STATE_ID):\n",
    "    arr_len = len(obj_arr)\n",
    "    Date_CST = []\n",
    "    Time_CST = []\n",
    "    for i in range(arr_len):\n",
    "        # iterate through\n",
    "        date_item = obj_arr['Date'][i]\n",
    "        time_item = obj_arr['Time'][i]\n",
    "        t = datetime.strptime(date_item + \" \" + time_item, \"%Y-%m-%d %H:%M\")\n",
    "        # Calculate the number of minutes past the last full hour\n",
    "        minutes_past_hour = t.minute + t.second / 60\n",
    "        # Round up to the next whole number of hours if the time is more than 30 minutes past the hour,\n",
    "        # or round down to the current hour if it's less than 30 minutes past\n",
    "        if minutes_past_hour >= 30:\n",
    "            num_hours = math.ceil(minutes_past_hour / 60)\n",
    "        else:\n",
    "            num_hours = 0\n",
    "        # Create a new datetime object representing the rounded time\n",
    "        if STATE_ID in ['IL', 'WI']:\n",
    "            # No need to dial back one hour\n",
    "            rounded_time_temp = t + timedelta(hours=num_hours)\n",
    "        else:\n",
    "            rounded_time_temp = t + timedelta(hours=num_hours-1)\n",
    "        rounded_time=datetime(year=rounded_time_temp.year,\n",
    "                         month=rounded_time_temp.month,\n",
    "                         day=rounded_time_temp.day,\n",
    "                         hour=rounded_time_temp.hour, minute=0, second=0)\n",
    "        result_stamp = rounded_time.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        \n",
    "        new_date, new_time = result_stamp.split(' ')\n",
    "        Date_CST.append(new_date)\n",
    "        Time_CST.append(new_time)\n",
    "        \n",
    "    obj_arr['Date_CST'] = Date_CST\n",
    "    obj_arr['Time_CST'] = Time_CST\n",
    "    return obj_arr\n",
    "\n",
    "##############################################################################\n",
    "def check_snow_24_120(df):\n",
    "    does_snow_24_120 = []\n",
    "    for i in range(len(df)):\n",
    "        if i + 120 < len(df):\n",
    "            if any(df['is_snow_precip'].iloc[i+24:i+120]):\n",
    "                does_snow_24_120.append(True)\n",
    "            else:\n",
    "                does_snow_24_120.append(False)\n",
    "        else:\n",
    "            does_snow_24_120.append(False)\n",
    "    df['does_snow_24_120'] = does_snow_24_120\n",
    "    return df\n",
    "    \n",
    "##############################################################################\n",
    "def lake_1D_matcher(df_temp, df_lake_1D_map):\n",
    "    # left join df_temp with df_lake_1D_map based on latitude and longitude\n",
    "    df_merged_temp = pd.merge(df_lake_1D_map, df_temp, on=['latitude', 'longitude'], how='left')\n",
    "\n",
    "    # extract value column into a list\n",
    "    value_temp = df_merged_temp['value'].tolist()\n",
    "\n",
    "    return value_temp\n",
    "    \n",
    "##############################################################################\n",
    "def is_valid_data(df_temp):\n",
    "    \n",
    "    crit_1 = 0\n",
    "    crit_2 = 0\n",
    "    crit_3 = 0\n",
    "        \n",
    "    # Pass in value\n",
    "    i_temp = df_temp.value\n",
    "    \n",
    "\n",
    "    try:\n",
    "        \n",
    "        i_temp_max = i_temp.max().max()\n",
    "\n",
    "        # Acquire mode in the array\n",
    "        i_temp_mode = i_temp.mode()[0]\n",
    "    except:\n",
    "        i_temp_max = 0\n",
    "        i_temp_mode = 0\n",
    "    \n",
    "    # Check Criteria #1:\n",
    "    if i_temp_max <= 0.03:\n",
    "        crit_1 = 1\n",
    "    \n",
    "    # Check Criteria #2:\n",
    "    if i_temp_mode <= 0.02:\n",
    "        crit_2 = 1\n",
    "    \n",
    "    # Check Criteria #3\n",
    "    if len(i_temp) <= 3000:\n",
    "        crit_3 = 1\n",
    "        \n",
    "    crit_sum = crit_1 + crit_2 + crit_3\n",
    "    \n",
    "    if crit_sum > 0:\n",
    "        cond = False\n",
    "    else:\n",
    "        cond = True\n",
    "        \n",
    "    return cond\n",
    "    \n",
    "    \n",
    "def cloud_finder(value_temp):\n",
    "    # Deduct 0.1 from all elements in the array\n",
    "    value_temp = [x - 0.1 for x in value_temp]\n",
    "    \n",
    "    # Count the number of elements that are larger than or equal to 0\n",
    "    count = sum(1 for x in value_temp if x >= 0)\n",
    "    \n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for start_year in start_year_lib:\n",
    "    ## It is always the case\n",
    "    end_year = start_year + 1\n",
    "\n",
    "    filename = weather_station[0:9]+str(start_year)+'Fall-'+str(end_year)+'Spring.csv'\n",
    "\n",
    "    ## Extract the state indicator\n",
    "    STATE_ID = filename[:2]\n",
    "    STATE_2_LTR = filename[:2]\n",
    "    # print(STATE_2_LTR)\n",
    "\n",
    "    ###########################################################################\n",
    "    # Check if they exist\n",
    "    if os.path.exists('NSW_Weather/'+weather_station+'/'):\n",
    "        print('Weather station data exists for ', weather_station)\n",
    "    else:\n",
    "        print('Weather station data does not exist for ', weather_station)\n",
    "        \n",
    "    if os.path.isfile('NSW_Weather/'+weather_station+'/'+filename):\n",
    "        print(\"File exists, reading table!\")\n",
    "        temp_table = pd.read_csv('NSW_Weather/'+weather_station+'/'+filename, skiprows = 8, skipfooter = 15)\n",
    "    else:\n",
    "        print(\"File does not exist, please rewind!\")\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Go get the satellite imagery data\n",
    "\n",
    "    folder_name = 'zone_0_'+filename[9:13]+'Fall_'+filename[18:22]+'Spring'\n",
    "    # print(folder_name)\n",
    "\n",
    "    # Acquire the folder location for 2-D lake data based on folder_name\n",
    "    folder_name_2D = folder_name[0:7]+'T_'+folder_name[7:]\n",
    "    # print(folder_name_2D)\n",
    "\n",
    "    # Add the parent folder name\n",
    "    parent_path = 'GOES_Hourly_Statistics/'\n",
    "\n",
    "    # Check if they exist\n",
    "    if os.path.exists(parent_path + folder_name):\n",
    "        print('1-D Lake Michigan data exists for ', folder_name[7:15], ' to ', folder_name[16:])\n",
    "    else:\n",
    "        print('1-D Lake Michigan data does not exist for ', folder_name[7:15], ' to ', folder_name[16:])\n",
    "        \n",
    "    if os.path.exists(parent_path + folder_name_2D):\n",
    "        print('2-D Lake Michigan data exists for ', folder_name_2D[9:17], ' to ', folder_name_2D[18:])\n",
    "    else:\n",
    "        print('2-D Lake Michigan data does not exist for ', folder_name[9:17], ' to ', folder_name_2D[18:])\n",
    "\n",
    "    ##########################################################################\n",
    "    # Start a new part\n",
    "#     print(type(start_year))\n",
    "#     print(type(start_year))\n",
    "#     print(type(start_year))\n",
    "#     print(type(start_year))\n",
    "    temp_start = int(start_year)\n",
    "    temp_end = int(end_year)\n",
    "    start_dt = dtdt(temp_start, 10, 1)\n",
    "    end_dt = dtdt(temp_end, 3, 31)\n",
    "    weather_date_theo = [dt.strftime(\"%Y%m%d\") for dt in daterange(start_dt, end_dt)]\n",
    "    csv_date_list = list(itertools.chain.from_iterable(itertools.repeat(x, 24) for x in weather_date_theo))\n",
    "    csv_time_list = hour_lib * len(weather_date_theo)\n",
    "    # Example usage\n",
    "    weather_with_CST = round_datetime_to_nearest_hour(temp_table, STATE_ID)\n",
    "#     weather_with_CST['Precip (in)'] = weather_with_CST['Precip (in)'].replace(['m', 'M'], 0).astype(float)\n",
    "    # Replace 'm' and 'M' values in 'Precip (in)' column\n",
    "    for i in range(len(weather_with_CST)):\n",
    "        if weather_with_CST.loc[i, 'Precip (in)'] in ['m', 'M']:\n",
    "            weather_with_CST.loc[i, 'Precip (in)'] = np.nan\n",
    "        else:\n",
    "            weather_with_CST.loc[i, 'Precip (in)'] = float(weather_with_CST.loc[i, 'Precip (in)'])\n",
    "            \n",
    "    weather_with_CST['Precip (in)'].fillna(0.00, inplace = True)\n",
    "    \n",
    "    # Create a new copy so we can fiddle around with the mask.\n",
    "    weather_with_CST['precip_work_zone'] = weather_with_CST['Precip (in)'].copy()\n",
    "    # Loop over all rows (except the last three rows to avoid IndexErrors)\n",
    "    for i in range(len(weather_with_CST) - 3):\n",
    "        # If current row and row at t+3 are larger than 0.01\n",
    "        if weather_with_CST.loc[i, 'precip_work_zone'] > 0.01 and weather_with_CST.loc[i+3, 'precip_work_zone'] > 0.01:\n",
    "            # Update all rows between t and t+3 where value equals to 0.00\n",
    "            weather_with_CST.loc[i:i+3, 'precip_work_zone'] = weather_with_CST.loc[i:i+3, 'precip_work_zone'].replace(0.00, 0.0001)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     weather_with_CST['Precip (in)'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "#     weather_with_CST['Precip (in)'] = weather_with_CST['Precip (in)'].replace(['m', 'M'], method='ffill').astype(float)\n",
    "#     weather_with_CST['Temp (F)'] = weather_with_CST['Temp (F)'].replace(['m', 'M'], 200).astype(int)\n",
    "    # Replace 'm' and 'M' with NaN values\n",
    "    weather_with_CST['Temp (F)'] = weather_with_CST['Temp (F)'].replace(['m', 'M'], float('nan'))\n",
    "\n",
    "    # Forward-fill the missing values with the last valid value\n",
    "    weather_with_CST['Temp (F)'] = weather_with_CST['Temp (F)'].fillna(method='ffill').astype(float)\n",
    "    is_snow_precip = ((weather_with_CST['Precip (in)'] > 0) & (weather_with_CST['Temp (F)']  <= 32)).tolist()\n",
    "    is_precip = (weather_with_CST['Precip (in)'] > 0).tolist()\n",
    "    weather_with_CST['is_snow_precip'] = is_snow_precip\n",
    "    weather_with_CST['is_precip'] = is_precip\n",
    "    weather_with_CST = check_snow_24_120(weather_with_CST)\n",
    "    # Initialize empty list to store results\n",
    "    # does_snow_24_120 = []\n",
    "\n",
    "    # # Iterate through each row in the 'is_snow_precip' column\n",
    "    # for i in range(len(weather_with_CST['is_snow_precip'])):\n",
    "        \n",
    "    #     # Check if the next 24 to 120 rows contain more than 2 True values\n",
    "    #     if i+120 < len(weather_with_CST['is_snow_precip']):\n",
    "    #         rolling_sum = weather_with_CST['is_snow_precip'][i+24:i+121].rolling(window=25).sum()\n",
    "    #         count_true = np.sum(rolling_sum > 2)\n",
    "    #     else:\n",
    "    #         count_true = 0\n",
    "        \n",
    "    #     # Append the result to the list\n",
    "    #     does_snow_24_120.append(count_true)\n",
    "\n",
    "    # # Add the list to the dataframe\n",
    "    # weather_with_CST['does_snow_24_120'] = does_snow_24_120\n",
    "    Date_CST = pd.Series(csv_date_list, name = 'Date_CST')\n",
    "    Time_CST = pd.Series(csv_time_list, name = 'Time_CST')\n",
    "\n",
    "    # Start to retrieve files\n",
    "    # Get a list of all files in the directory\n",
    "    file_list_1D = os.listdir(parent_path + folder_name)\n",
    "\n",
    "    # Sort the list of files (Necessary on Linux)\n",
    "    file_list_1D.sort()\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    file_list_2D = os.listdir(parent_path + folder_name_2D)\n",
    "\n",
    "    # Sort the list of files (Necessary on Linux)\n",
    "    file_list_2D.sort()\n",
    "\n",
    "    intend_date_list = []\n",
    "\n",
    "    for date in csv_date_list:\n",
    "        intend_date = date[:4] + '.' + date[4:6] + '.' + date[6:]\n",
    "        intend_date_list.append(intend_date)\n",
    "    \n",
    "    intend_time_list = [t.replace(':', '') for t in csv_time_list]\n",
    "    intend_timestamp_list = [f\"{date}.{time}\" for date, time in zip(intend_date_list, intend_time_list)]\n",
    "    \n",
    "    Date_UTC = []\n",
    "\n",
    "    for date in csv_date_list:\n",
    "        intend_date = date[:4] + '-' + date[4:6] + '-' + date[6:]\n",
    "        Date_UTC.append(intend_date)\n",
    "        \n",
    "    Time_UTC = csv_time_list.copy()\n",
    "    \n",
    "    file_1D_timestamp = []\n",
    "\n",
    "    for filename in file_list_1D:\n",
    "        file_1D = filename[7:22]\n",
    "        file_1D_timestamp.append(file_1D)\n",
    "    \n",
    "    # Define UTC and CST time zones\n",
    "    utc_tz = timezone('UTC')\n",
    "    cst_tz = timezone('Etc/GMT+6')\n",
    "\n",
    "    # Example lists of datetime strings\n",
    "    date_utc_list = Date_UTC.copy()\n",
    "    time_utc_list = Time_UTC.copy()\n",
    "\n",
    "    # Convert datetime strings from UTC to CST\n",
    "    date_cst_list = []\n",
    "    time_cst_list = []\n",
    "    for date_str, time_str in zip(date_utc_list, time_utc_list):\n",
    "        datetime_utc = datetime.strptime(date_str + ' ' + time_str, '%Y-%m-%d %H:%M')\n",
    "        datetime_utc = utc_tz.localize(datetime_utc)\n",
    "        datetime_cst = datetime_utc.astimezone(cst_tz)\n",
    "        date_cst_list.append(datetime_cst.strftime('%Y-%m-%d'))\n",
    "        time_cst_list.append(datetime_cst.strftime('%H:%M'))\n",
    "        \n",
    "    data_dict = {'Date_UTC': Date_UTC,\n",
    "             'Time_UTC': Time_UTC,\n",
    "             'Date_CST': date_cst_list,\n",
    "             'Time_CST': time_cst_list,\n",
    "             'intend_timestamp_list': intend_timestamp_list}\n",
    "\n",
    "    # create the DataFrame using the dictionary\n",
    "    df_GOES_time_lib = pd.DataFrame(data_dict)\n",
    "    \n",
    "    df_GOES_file_lib = pd.DataFrame({'file_timestamp': file_1D_timestamp, 'file_list_1D': file_list_1D, 'file_list_2D': file_list_2D})\n",
    "    \n",
    "    df_GOES_combined = pd.merge(df_GOES_time_lib, df_GOES_file_lib, left_on='intend_timestamp_list', right_on='file_timestamp', how='left')\n",
    "    df_GOES_combined.fillna('None', inplace=True)\n",
    "    \n",
    "    output_dir = 'output/GOES_file_lib_dir/'\n",
    "    output_csv_name = str(start_year)+'Fall_'+str(end_year)+'Spring_GOES_lib.csv'\n",
    "\n",
    "    output_file_path = os.path.join(output_dir, output_csv_name)\n",
    "\n",
    "    df_GOES_combined.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    ###################\n",
    "    df_reference_1D = pd.read_csv('02-05-2023/zone_0_sample_take_2/' + 'goes15.2016.12.12.1700.v01.nc-var1-t0.csv')\n",
    "    table_1D_len = df_reference_1D.shape[0]\n",
    "    df_lake_1D_map = df_reference_1D[['latitude', 'longitude']].copy()\n",
    "    matched_1D_file_list = df_GOES_combined['file_list_1D'].tolist()\n",
    "    matched_2D_file_list = df_GOES_combined['file_list_2D'].tolist()\n",
    "    matrix = df_reference_1D.values\n",
    "    df_tester_again = pd.read_csv('02-05-2023/zone_0_sample_take_2/' + 'goes15.2016.12.08.1200.v01.nc-var1-t0.csv')\n",
    "    temp_result_tester = lake_1D_matcher(df_tester_again, df_lake_1D_map)\n",
    "    \n",
    "    ###################\n",
    "    lake_1D_list = []\n",
    "    # lat_lists = []\n",
    "    # lon_lists = []\n",
    "    cond_list = []\n",
    "    count_list = []\n",
    "    cloud_exist_list = []\n",
    "    index_pack = [np.array(i) for i in range(len(matched_1D_file_list))]\n",
    "    counter_checker = 0\n",
    "#     attempt_file_list = df_GOES_combined['file_attemption'].tolist()\n",
    "#     for file_name in tqdm(matched_1D_file_list):\n",
    "    for idx_cursor in tqdm(index_pack):\n",
    "        file_name = matched_1D_file_list[idx_cursor]\n",
    "    # for file_name in matched_1D_file_list[1769:1775]:\n",
    "        \n",
    "        try:\n",
    "            temp_file_path = parent_path + folder_name + '/' + file_name\n",
    "    #         print( temp_file_path)\n",
    "    #         print(counter)\n",
    "            df_temp = pd.read_csv(temp_file_path)\n",
    "            value_temp = lake_1D_matcher(df_temp, df_lake_1D_map)\n",
    "            cond = is_valid_data(df_temp)\n",
    "            cond_list.append(cond)\n",
    "            \n",
    "            num_clouds = cloud_finder(value_temp)\n",
    "            count_list.append(num_clouds) \n",
    "            if num_clouds < 720:\n",
    "                exist_temp = False\n",
    "            else:\n",
    "                exist_temp = True\n",
    "            # Replace all NaN values in the 'exist_temp' array with 0.0\n",
    "            exist_temp = np.nan_to_num(exist_temp, nan=0.0)\n",
    "            cloud_exist_list.append(exist_temp)\n",
    "            # Replace NaN values with 0.0\n",
    "            value_temp = [0.0 if np.isnan(val) else val for val in value_temp]\n",
    "            lake_1D_list.append(value_temp)\n",
    "    #         lat_lists.append(lat_list)\n",
    "    #         lon_lists.append(lon_list)\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                temp_timer_stamper = intend_timestamp_list[idx_cursor]\n",
    "                timer_stamper = temp_timer_stamper[:-2] + '15'\n",
    "                temp_file_dir = parent_path + folder_name\n",
    "                # Iterate over files in the directory\n",
    "                for filename in os.listdir(temp_file_dir):\n",
    "                    if filename.endswith('.csv') and timer_stamper in filename:\n",
    "                        temp_file_path = os.path.join(temp_file_dir, filename)\n",
    "                        counter_checker = counter_checker + 1\n",
    "                        df_GOES_combined.at[int(idx_cursor), 'file_list_1D'] = str(filename)\n",
    "#                         print(temp_file_path)\n",
    "                        break  # Stop iterating after finding the first matching file\n",
    "#                 else:\n",
    "#                     temp_file_path = None\n",
    "#                 file_name = file_name.replace('00.v01', '15.v01')\n",
    "#                 temp_file_path = parent_path + folder_name + '/' + filename\n",
    "        #         print( temp_file_path)\n",
    "        #         print(counter)\n",
    "#                 print(temp_file_path)\n",
    "                df_temp = pd.read_csv(temp_file_path)\n",
    "                \n",
    "#                 print(df_temp)\n",
    "                value_temp = lake_1D_matcher(df_temp, df_lake_1D_map)\n",
    "                cond = is_valid_data(df_temp)\n",
    "                cond_list.append(cond)\n",
    "#                 lake_1D_list.append(value_temp)\n",
    "                num_clouds = cloud_finder(value_temp)\n",
    "                count_list.append(num_clouds) \n",
    "                if num_clouds < 720:\n",
    "                    exist_temp = False\n",
    "                else:\n",
    "                    exist_temp = True\n",
    "                # Replace all NaN values in the 'exist_temp' array with 0.0\n",
    "                exist_temp = np.nan_to_num(exist_temp, nan=0.0)\n",
    "                cloud_exist_list.append(exist_temp)\n",
    "                # Replace NaN values with 0.0\n",
    "                value_temp = [0.0 if np.isnan(val) else val for val in value_temp]\n",
    "                lake_1D_list.append(value_temp)\n",
    "            except FileNotFoundError:\n",
    "                lake_1D_list.append(np.zeros(3599))\n",
    "                cond_list.append(False)\n",
    "                count_list.append(0) \n",
    "                cloud_exist_list.append(False)\n",
    "                \n",
    "#         except FileNotFoundError:\n",
    "#             lake_1D_list.append(np.zeros(3599))\n",
    "#             cond_list.append(False)\n",
    "#             count_list.append(0) \n",
    "#             cloud_exist_list.append(False)\n",
    "    \n",
    "    # list of matrices\n",
    "    lake_2D_list = []\n",
    "\n",
    "#     # loop over file names\n",
    "#     for file_name in tqdm(matched_2D_file_list):\n",
    "#         try:\n",
    "#             # read csv file into dataframe\n",
    "#             temp_file_path = parent_path + folder_name_2D + '/' + file_name\n",
    "#             df_temp = pd.read_csv(temp_file_path)\n",
    "#             df_temp = df_temp.iloc[1:, 1:]\n",
    "#             # Replace NaN values in 'values' with 0 in 'df_temp'\n",
    "#             df_temp = df_temp.fillna(0)\n",
    "#     #         print(df_temp.shape)\n",
    "#             # convert dataframe to numpy array/matrix\n",
    "#             mat_temp = df_temp.values\n",
    "#     #         mat_temp = np.array(df_temp.values.flatten())\n",
    "\n",
    "#             # assume df_temp is a DataFrame with multiple columns\n",
    "#     #         arrays = [df_temp.iloc[:, i].to_numpy() for i in range(len(df_temp.columns))]\n",
    "#     #         mat_temp = arrays\n",
    "#         except FileNotFoundError:\n",
    "#             # if file does not exist, save NaN\n",
    "#             mat_temp = np.zeros((105, 79))\n",
    "        \n",
    "#         # append matrix to list\n",
    "#         lake_2D_list.append(mat_temp)\n",
    "        \n",
    "    df_GOES_combined['lake_1D_list'] = lake_1D_list\n",
    "#     df_GOES_combined['lake_2D_list'] = lake_2D_list\n",
    "    df_GOES_combined['data_usable'] = cond_list\n",
    "    df_GOES_combined['cloud_count'] = count_list\n",
    "    df_GOES_combined['cloud_exist'] = cloud_exist_list\n",
    "    \n",
    "    df_GOES_meteo_combined = pd.merge(df_GOES_combined, weather_with_CST, on=['Date_CST', 'Time_CST'], how='left')\n",
    "    \n",
    "    ################################################################\n",
    "    \n",
    "    time_list = ['21:00', '22:00', '23:00', '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', '10:00', '11:00', '12:00', '13:00']\n",
    "\n",
    "    for i in range(2, len(df_GOES_meteo_combined)):\n",
    "        current_time = df_GOES_meteo_combined.loc[i, 'Time_UTC']\n",
    "        if current_time in time_list:\n",
    "            if df_GOES_meteo_combined.loc[i-1, 'cloud_count'] != 0:\n",
    "                df_GOES_meteo_combined.loc[i, 'cloud_count'] = df_GOES_meteo_combined.loc[i-1, 'cloud_count']\n",
    "            else:\n",
    "                df_GOES_meteo_combined.loc[i, 'cloud_count'] = df_GOES_meteo_combined.loc[i-2, 'cloud_count']\n",
    "        else:\n",
    "            if df_GOES_meteo_combined.loc[i, 'cloud_count'] == 0:\n",
    "                df_GOES_meteo_combined.loc[i, 'cloud_count'] = df_GOES_meteo_combined.loc[i-1, 'cloud_count']\n",
    "\n",
    "    ################################################################\n",
    "    \n",
    "#     time_utc_list = ['21:00', '22:00', '23:00', '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', '10:00', '11:00', '12:00', '13:00']\n",
    "#     last_cloud_count = 0\n",
    "\n",
    "#     # Iterate over the rows of the dataframe\n",
    "#     for index, row in df_GOES_combined.iterrows():\n",
    "#         if row['Time_UTC'] in time_utc_list:\n",
    "#             if last_cloud_count > 0:\n",
    "#                 df_GOES_combined.at[index, 'cloud_count'] = last_cloud_count\n",
    "#         elif row['cloud_count'] > 0:\n",
    "#             last_cloud_count = row['cloud_count']\n",
    "    \n",
    "    column_names = df_GOES_meteo_combined.columns.tolist()\n",
    "    \n",
    "    df_GOES_meteo_combined = df_GOES_meteo_combined.drop(['Date', 'Time', 'intend_timestamp_list', 'file_timestamp'], axis=1)\n",
    "    \n",
    "    df_GOES_meteo_combined.rename(columns={'file_list_1D': 'File_name_for_1D_lake'}, inplace=True)\n",
    "    df_GOES_meteo_combined.rename(columns={'file_list_2D': 'File_name_for_2D_lake'}, inplace=True)\n",
    "    df_GOES_meteo_combined.rename(columns={'lake_1D_list': 'Lake_data_1D'}, inplace=True)\n",
    "#     df_GOES_meteo_combined.rename(columns={'lake_2D_list': 'Lake_data_2D'}, inplace=True)\n",
    "    \n",
    "    column_names = df_GOES_meteo_combined.columns.tolist()\n",
    "    # Concatenate the temporary data to the 'df_combined_all' dataframe\n",
    "    df_combined_all = pd.concat([df_combined_all, df_GOES_meteo_combined], ignore_index=True)\n",
    "    \n",
    "    # Store the length of 'df_GOES_meteo_combined' in the 'lengths_list'\n",
    "    lengths_list.append(len(df_GOES_meteo_combined))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd698577",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'output/'\n",
    "output_csv_name = 'Take_2_'+str(start_year_lib[0])+'Fall_'+str(end_year)+'Spring_GOES_meteo_combined_'+station_ID_num+'.csv'\n",
    "\n",
    "output_file_path = os.path.join(output_dir, output_csv_name)\n",
    "\n",
    "df_combined_all.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1903c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, Dense, Flatten, Dropout\n",
    "\n",
    "# X = np.stack(df_combined_all['Lake_data_1D'].to_numpy())\n",
    "\n",
    "# df_combined_all['is_snow_precip'] = df_combined_all['is_snow_precip'].apply(lambda x: int(round(x)) if isinstance(x, float) and not np.isnan(x) else (int(x) if not np.isnan(x) else 0))\n",
    "\n",
    "# y = df_combined_all['is_snow_precip'].values.astype(int)\n",
    "# # print(y)\n",
    "\n",
    "# # Fill NaN values with 0\n",
    "# X = np.nan_to_num(X)\n",
    "# y = np.nan_to_num(y)\n",
    "\n",
    "\n",
    "# input_data = []\n",
    "# output_data = []\n",
    "\n",
    "# for i in range(len(X) - 120):\n",
    "#     input_data.append(X[i:i+72])\n",
    "#     output_data.append(y[i+120])\n",
    "\n",
    "# input_data = np.stack(input_data)\n",
    "# output_data = np.stack(output_data)\n",
    "\n",
    "\n",
    "# # Scale the input data\n",
    "# scaler = StandardScaler()\n",
    "# input_data_scaled = scaler.fit_transform(input_data.reshape(input_data.shape[0], -1)).reshape(input_data.shape)\n",
    "\n",
    "# # Reshape the input data to match Conv1D input shape (batch_size, steps, input_dim)\n",
    "# input_data_scaled = input_data_scaled.reshape(input_data_scaled.shape[0], input_data_scaled.shape[1], input_data_scaled.shape[2])\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(input_data_scaled, output_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# model.add(Conv1D(64, 3, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mae', 'accuracy'])\n",
    "\n",
    "\n",
    "# # model.compile(optimizer='adam', loss='mse', metrics=['mae', 'accuracy'])\n",
    "\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1713d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOP\n"
     ]
    }
   ],
   "source": [
    "print('EOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521ab7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f51c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052e950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
